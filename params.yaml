XG_Boost:
#    n_estimators: [50, 100, 150, 200, 250, 300, 400, 500]
#    max_depth: [5, 10, 15, 20, 25, 30, 40, 50]
#    learning_rate: [0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 1]
#    subsample: [0.1, 0.5, 0.7, 0.8, 0.3, 0.4, 0.6, 0.2]
    n_estimators: [50, 100, 150, 200, 500]
    max_depth: [5, 10, 15, 20, 30]
    learning_rate: [0.01, 0.1, 0.3, 0.5, 0.7]
    subsample: [0.1, 0.5, 0.7, 0.8, 0.2]
    eta: [ 0.3, 0.1, 0.01, 0.4, 0,6]
    colsample_bytree: [0.1, 0.5, 0.7, 0.8, 0.2]
    'reg_alpha': [0.001, 0.01, 0.1, 0.5, 1.0]
#    n_estimators: 200
#    max_depth: 10
#    learning_rate : 0.3
#    subsample: 1.0
#    colsample_bytree: 0.9


#base_score=0.5, booster='gbtree',
                #                               n_estimators=200,
                #                               # early_stopping_rounds=50,
                #                               objective='reg:squarederror',
                #                               max_depth=6,
                #                               learning_rate=0.01,
                #                                min_child_weight=1,
                #                               subsample=0.8,
                #                               colsample_bytree=0.8,
                #                               gamma=0,
                #                               reg_alpha=0,
                #                               reg_lambda=1